{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example for the `Speech-To-Text (STT)` API\n",
    "\n",
    "This notebook illustrates how to build a pretrained `Whisper` model, and use it to transcribe an audio file !\n",
    "\n",
    "Models are converted, from either the official `openai` checkpoints, either from the `transformers` models checkpoints, from `pytorch` to a `keras` instance. For this purpose, both your keras backend, and `torch` have to be installed. Once the model has been converted, `torch` is not required anymore ;) \n",
    "\n",
    "The `tts` method works on both audios and videos !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build whisper\n",
    "\n",
    "[Whisper](https://github.com/openai/whisper) is a multilingual `Speech-to-Text` model trained by `OpenAI`.\n",
    "\n",
    "**Note** : `pytorch` is required to convert the official `pytorch` checkpoint to a `keras` checkpoint ;)\n",
    "\n",
    "**Note** : the tokenizer is now copied from the `transformers` library, as the new official `openai`'s code is using their custom `tiktoken` tokenizer. This means that the 2 tokenizers are not *exactly* identical, but are compatible as the differences do not have any impact on the model.\n",
    "\n",
    "There is 2 supported versions of `Whisper` for inference :\n",
    "- The `keras` implementation provided in `architectures/transformers/whisper_arch.py`\n",
    "- The [TensorRT-LLM](https://github.com/NVIDIA/TensorRT-LLM) implementation\n",
    "\n",
    "To get the best inference performances, the second option is the best one. The first implementation in `keras` is still maintained as it is a good way to effectively understand how `Whisper` is implemented, and working. It is worth mentioning that the provided `keras` implementation is faster than the `pytorch` implementation from the `transformers` library, as it leverages the `tensorflow XLA` optimization !\n",
    "\n",
    "The convertion to `TensorRT-LLM` is however a bit more complex, and require some additional configurations. Check the [installation guide](https://github.com/yui-mhcp/blob/master/INSTALLATION.md) to properly configure the libraries, then follow the steps below to build the TensorRT-LLM engine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convertion to `keras`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========== whisper-base ==========\n",
      "Model :\n",
      "- Inputs \t: unknown\n",
      "- Outputs \t: unknown\n",
      "- Number of layers \t: 2\n",
      "- Number of parameters \t: 71.826 Millions\n",
      "- Model not compiled yet\n",
      "\n",
      "Transfer-learning from : base\n",
      "Already trained on 0 epochs (0 steps)\n",
      "\n",
      "- Language : multi\n",
      "- Vocabulary (size = 50364) : ['!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ...]\n",
      "- Audio rate : 16000\n",
      "- # mel channels : 80\n",
      "- Use CTC decoder : False\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from models.stt import Whisper\n",
    "\n",
    "model = Whisper(pretrained = 'base', lang = 'multi', nom = 'whisper-base')\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convertion to `TensorRT-LLM`\n",
    "\n",
    "**WARNING** The `TensorRT-LLM` engines are specific to a version of the library, meaning that if you update the `TensorRT-LLM` library, you will have to re-build the engine. Furthermore, the current `convert_checkpoint` scripts in the official `TensorRT-LLM` repository only supports official `Whisper` implementations by `OpenAI`, and not the versions from the `transformers` library. This is the reason why I provide a custom `convert_checkpoint.py` script that supports these custom models ! However, this is currently only working for the version `0.15.0`, and is not officially maintained accross versions.\n",
    "\n",
    "In the future, my objective is to make a PR to the [TensorRT-LLM repository](https://github.com/NVIDIA/TensorRT-LLM) to propose this more general convertion script ! My plan is to make it once the `v0.18.0` is released to make it compatible with the latest version."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1 : loading the pre-trained pytorch version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSpeechSeq2Seq\n",
    "\n",
    "model_name = 'bofenghuang/whisper-large-v3-french'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model     = AutoModelForSpeechSeq2Seq.from_pretrained(model_name, device_map = 'cuda', torch_dtype = 'float16')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2 : build the `TensorRT-LLM` engine\n",
    "\n",
    "The below script displays the commands to execute in the `TensorRT-LLM/examples/whisper` directory.\n",
    "\n",
    "1. Clone the official repository\n",
    "\n",
    "`git clone https://github.com/NVIDIA/TensorRT-LLM --branch v0.15.0.`\n",
    "\n",
    "2. Navigate to the directory, and copy the provided weights convertion script\n",
    "\n",
    "```bash\n",
    "cp convert_checkpoint.py TensorRT-LLM/examples/whisper/.\n",
    "cd TensorRT-LLM/examples/whisper\n",
    "```\n",
    "\n",
    "3. Set the desired model to convert, then display the commands to execute\n",
    "4. Run the commands in your python environment\n",
    "\n",
    "**Note** : the parameters in the below commands have been optimized to use the less possible memory in a single inference use-case. If you plan to use the model via an API server, you may need to increase the batch_size to support multiple requests in parallel !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "model_name = 'bofenghuang/whisper-large-v3-french'\n",
    "\n",
    "if 'openai' not in model_name:\n",
    "    hf_path = os.path.dirname(\n",
    "        glob.glob(os.path.expanduser('~/.cache/huggingface/hub/models--{}/snapshots/**/config.json'.format(model_name.replace('/', '--'))))[0]\n",
    "    )\n",
    "else:\n",
    "    raise NotImplementedError()\n",
    "\n",
    "model_path = model_name.split('/')[1].replace('-', '_')\n",
    "\n",
    "int8 = True\n",
    "batch_size = 1\n",
    "\n",
    "if int8: model_path += '_int8'\n",
    "\n",
    "cmd1 = \"\"\"\n",
    "python convert_checkpoint.py \\\\\n",
    "    --model_dir {} \\\\\n",
    "    --output_dir {}_checkpoint \\\\\n",
    "    --dtype float16\n",
    "\"\"\".format(hf_path, model_path)\n",
    "if int8: cmd1 = cmd1.strip() + \"\\\\\\n    --use_weight_only \\\\\\n    --weight_only_precision int8\"\n",
    "\n",
    "cmd2 = \"\"\"\n",
    "trtllm-build --checkpoint_dir {model_path}_checkpoint/encoder \\\\\n",
    "             --output_dir {model_path}_engine/encoder \\\\\n",
    "             --max_batch_size {batch_size} \\\\\n",
    "             --max_seq_len 3000 \\\\\n",
    "             --max_input_len 3000 \\\\\n",
    "             --max_encoder_input_len 3000 \\\\\n",
    "             --kv_cache_type disabled\n",
    "\"\"\".format(model_path = model_path, batch_size = batch_size)\n",
    "\n",
    "cmd3 = \"\"\"\n",
    "trtllm-build --checkpoint_dir {model_path}_checkpoint/decoder \\\\\n",
    "             --output_dir {model_path}_engine/decoder \\\\\n",
    "             --max_batch_size {batch_size} \\\\\n",
    "             --max_beam_width 5 \\\\\n",
    "             --max_encoder_input_len 3000 \\\\\n",
    "             --max_input_len 4 \\\\\n",
    "             --max_seq_len 512 \\\\\n",
    "             --tokens_per_block 16\n",
    "\"\"\".format(model_path = model_path, batch_size = batch_size)\n",
    "\n",
    "print(cmd1)\n",
    "print(cmd2)\n",
    "print(cmd3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3 : building the `Whisper` instance\n",
    "\n",
    "The `BaseModel` class (the base class for the `Whisper` implementation) supports the `TensorRT-LLM` runtime, by specifying the `runtime` argument (in this case : `trt_llm`). This runtime requires the `path` argument, specifying the path to the runtime engine. The `kv_cache_free_gpu_memory` is a convenient feature I have added, which internally uses the `kv_cache_free_gpu_memory_fraction` from the `TRT-LLM` framework, limiting the memory used by the model buffers. This is based on estimations to convert the absolute value to the *free_memory_fraction*, but is still more convenient than manually estimating the fraction ;)\n",
    "\n",
    "**Important note** : the `TensorRT-LLM` implementation uses a custom GPU memory manager, it is the reason why it is important to limit the visible gpu memory for `tensorflow`, to leave spaces to `TensorRT-LLM` ! `tensorflow` is used to compute the mel-spectrogram (the input of `whisper`), and therefore requires to be initialized. Another solution is to switch the `keras` backend to `pytorch`, but it is not always possible nor profitable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory limited to 512Mb\n",
      "[TensorRT-LLM][INFO] Engine version 0.15.0 found in the config file, assuming engine(s) built by new builder API.\n",
      "[TensorRT-LLM][INFO] Engine version 0.15.0 found in the config file, assuming engine(s) built by new builder API.\n",
      "[TensorRT-LLM][INFO] Setting encoder max input length and hidden size for accepting visual features.\n",
      "[TensorRT-LLM][INFO] MPI size: 1, MPI local size: 1, rank: 0\n",
      "[TensorRT-LLM][INFO] Engine version 0.15.0 found in the config file, assuming engine(s) built by new builder API.\n",
      "[TensorRT-LLM][INFO] Setting encoder max input length and hidden size for accepting visual features.\n",
      "[TensorRT-LLM][INFO] Refreshed the MPI local session\n",
      "[TensorRT-LLM][INFO] Engine version 0.15.0 found in the config file, assuming engine(s) built by new builder API.\n",
      "[TensorRT-LLM][INFO] MPI size: 1, MPI local size: 1, rank: 0\n",
      "[TensorRT-LLM][INFO] TRTGptModel maxNumSequences: 1\n",
      "[TensorRT-LLM][INFO] TRTGptModel maxBatchSize: 1\n",
      "[TensorRT-LLM][INFO] TRTGptModel maxBeamWidth: 1\n",
      "[TensorRT-LLM][INFO] TRTGptModel maxSequenceLen: 3000\n",
      "[TensorRT-LLM][INFO] TRTGptModel maxDraftLen: 0\n",
      "[TensorRT-LLM][INFO] TRTGptModel mMaxAttentionWindowSize: (3000) * 32\n",
      "[TensorRT-LLM][INFO] TRTGptModel enableTrtOverlap: 0\n",
      "[TensorRT-LLM][INFO] TRTGptModel normalizeLogProbs: 1\n",
      "[TensorRT-LLM][INFO] TRTGptModel maxNumTokens: 3000\n",
      "[TensorRT-LLM][INFO] TRTGptModel maxInputLen: 2999 = min(maxSequenceLen - 1, maxNumTokens) since context FMHA and usePackedInput are enabled\n",
      "[TensorRT-LLM][INFO] TRTGptModel If model type is encoder, maxInputLen would be reset in trtEncoderModel to maxInputLen: min(maxSequenceLen, maxNumTokens).\n",
      "[TensorRT-LLM][INFO] Capacity Scheduler Policy: GUARANTEED_NO_EVICT\n",
      "[TensorRT-LLM][INFO] Context Chunking Scheduler Policy: None\n",
      "[TensorRT-LLM][INFO] Loaded engine size: 626 MiB\n",
      "[TensorRT-LLM][INFO] Inspecting the engine to identify potential runtime issues...\n",
      "[TensorRT-LLM][INFO] The profiling verbosity of the engine does not allow this analysis to proceed. Re-build the engine with 'detailed' profiling verbosity to get more diagnostics.\n",
      "[TensorRT-LLM][INFO] [MemUsageChange] Allocated 53.97 MiB for execution context memory.\n",
      "[TensorRT-LLM][INFO] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +0, now: CPU 0, GPU 619 (MiB)\n",
      "[TensorRT-LLM][INFO] TRTEncoderModel mMaxInputLen: reset to 3000 from build config.\n",
      "[TensorRT-LLM][INFO] MPI size: 1, MPI local size: 1, rank: 0\n",
      "[TensorRT-LLM][INFO] Rank 0 is using GPU 0\n",
      "[TensorRT-LLM][INFO] TRTGptModel maxNumSequences: 1\n",
      "[TensorRT-LLM][INFO] TRTGptModel maxBatchSize: 1\n",
      "[TensorRT-LLM][INFO] TRTGptModel maxBeamWidth: 5\n",
      "[TensorRT-LLM][INFO] TRTGptModel maxSequenceLen: 512\n",
      "[TensorRT-LLM][INFO] TRTGptModel maxDraftLen: 0\n",
      "[TensorRT-LLM][INFO] TRTGptModel mMaxAttentionWindowSize: (512) * 32\n",
      "[TensorRT-LLM][INFO] TRTGptModel enableTrtOverlap: 0\n",
      "[TensorRT-LLM][INFO] TRTGptModel normalizeLogProbs: 1\n",
      "[TensorRT-LLM][INFO] TRTGptModel maxNumTokens: 512\n",
      "[TensorRT-LLM][INFO] TRTGptModel maxInputLen: 511 = min(maxSequenceLen - 1, maxNumTokens) since context FMHA and usePackedInput are enabled\n",
      "[TensorRT-LLM][INFO] TRTGptModel If model type is encoder, maxInputLen would be reset in trtEncoderModel to maxInputLen: min(maxSequenceLen, maxNumTokens).\n",
      "[TensorRT-LLM][INFO] Capacity Scheduler Policy: GUARANTEED_NO_EVICT\n",
      "[TensorRT-LLM][INFO] Context Chunking Scheduler Policy: None\n",
      "[TensorRT-LLM][INFO] The logger passed into createInferRuntime differs from one already provided for an existing builder, runtime, or refitter. Uses of the global logger, returned by nvinfer1::getLogger(), will return the existing value.\n",
      "[TensorRT-LLM][INFO] Loaded engine size: 940 MiB\n",
      "[TensorRT-LLM][INFO] Inspecting the engine to identify potential runtime issues...\n",
      "[TensorRT-LLM][INFO] The profiling verbosity of the engine does not allow this analysis to proceed. Re-build the engine with 'detailed' profiling verbosity to get more diagnostics.\n",
      "[TensorRT-LLM][INFO] [MemUsageChange] Allocated 68.69 MiB for execution context memory.\n",
      "[TensorRT-LLM][INFO] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +0, now: CPU 0, GPU 1549 (MiB)\n",
      "[TensorRT-LLM][INFO] [MemUsageChange] Allocated 7.89 MB GPU memory for runtime buffers.\n",
      "[TensorRT-LLM][INFO] [MemUsageChange] Allocated 2.77 MB GPU memory for decoder.\n",
      "[TensorRT-LLM][INFO] Memory usage when calculating max tokens in paged kv cache: total: 23.68 GiB, available: 21.71 GiB\n",
      "[TensorRT-LLM][INFO] Number of blocks in KV cache primary pool: 201\n",
      "[TensorRT-LLM][INFO] Number of blocks in KV cache secondary pool: 0, onboard blocks to primary memory before reuse: true\n",
      "[TensorRT-LLM][INFO] Max KV cache pages per sequence: 32\n",
      "[TensorRT-LLM][INFO] Number of tokens per block: 16.\n",
      "[TensorRT-LLM][INFO] [MemUsageChange] Allocated 0.24 GiB for max tokens in paged KV cache (1600).\n",
      "[TensorRT-LLM][INFO] Max KV cache pages per sequence: 188\n",
      "[TensorRT-LLM][INFO] Number of tokens per block: 16.\n",
      "[TensorRT-LLM][INFO] [MemUsageChange] Allocated 0.24 GiB for max tokens in paged KV cache (1600).\n",
      "[TensorRT-LLM][INFO] This is an Encoder-Decoder model, set 0.5 cross KV cache fraction based on the config.\n",
      "[TensorRT-LLM][INFO] Number of blocks in self KV cache primary pool: 100, in cross KV cache primary pool: 100\n",
      "[TensorRT-LLM][INFO] Number of blocks in self KV cache secondary pool: 0, in cross KV cache secondary pool: 0\n",
      "[TensorRT-LLM][INFO] Enable MPI KV cache transport.\n",
      "Whisper `trt_llm_whisper_large_v3_french_int8` initialized successfully !\n",
      "\n",
      "========== trt_llm_whisper_large_v3_french_int8 ==========\n",
      "Model :\n",
      "<TensorRTLLMRuntime path=../../libs/TensorRT-LLM/examples/whisper/whisper_large_v3_french_int8_engine>\n",
      "\n",
      "Transfer-learning from : bofenghuang/whisper-large-v3-french\n",
      "- Language : en\n",
      "- Vocabulary (size = 50364) : ['!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ...]\n",
      "- Audio rate : 16000\n",
      "- # mel channels : 128\n",
      "- Use CTC decoder : False\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from loggers import *\n",
    "from models.stt import Whisper\n",
    "from utils import setup_environment\n",
    "\n",
    "setup_environment(gpu_memory = 512)\n",
    "\n",
    "int8 = True\n",
    "\n",
    "model_name = 'bofenghuang/whisper-large-v3-french'\n",
    "model_path = model_name.split('/')[1].replace('-', '_')\n",
    "if int8: model_path += '_int8'\n",
    "\n",
    "path = '../../libs/TensorRT-LLM/examples/whisper/{}_engine'.format(model_path)\n",
    "\n",
    "model = Whisper(\n",
    "    lang = 'multi',\n",
    "    name = 'trt_llm_' + model_path,\n",
    "    tokenizer  = 'openai/whisper-medium', # all the tokenizers are the same in the Whisper family of models\n",
    "    pretrained = model_name,\n",
    "    \n",
    "    path = path,\n",
    "    runtime = 'trt_llm',\n",
    "    kv_cache_free_gpu_memory = 512\n",
    ")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction API\n",
    "\n",
    "The prediction API is very simple to use : pass the audio filename(s), and the model you want (or the audio language) and that's it !\n",
    "\n",
    "The prediction splits the audio into *chunks* of a given amount of time (default to 30sec), and predicts the text for each chunk. Then it concatenates all the texts to build the complete transcription of the audio file ! Note that `Whisper` also splits each chunk into frames corresponding to the timestamps of the transcription (these are provided in the output). This may be useful to search a span of text, or even complete / correct the transcription ! \n",
    "\n",
    "This demonstration is performed on a short and clean audio. Nevertheless, `Whisper` has been trained on large scale datasets, and is able to transcribe audios in many languages, even in noisy or low quality audios !\n",
    "\n",
    "`Whisper` is a multilingual model, meaning that it can transcribe audios from a large variety of languages. However, it has to know which language it should use. To this end, the 1st prediction step is to detect the language from the audio. To skip this part, and thus speed up the prediction time, you can provide the `lang` argument ;)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timers :\n",
      "- predict : 45 ms\n",
      "  - predict : 45 ms\n",
      "    - inference : 45 ms\n",
      "      - loading audio : 23 ms\n",
      "        - read_audio : 21 ms\n",
      "          - read file : 20 ms\n",
      "          - normalize_audio : 427 μs\n",
      "          - trim_silence : 216 μs\n",
      "        - mel spectrogram : 1.447 ms\n",
      "          - graph_mel_spectrogram : 806 μs\n",
      "      - segment processing : 514 μs\n",
      "      - xla_infer : 20 ms\n",
      "      - post_processing : 106 μs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'text': '<|notimestamps|> The streets were narrow and unpaid but very fairly clean.',\n",
       "  'segments': [{'start': 0.0,\n",
       "    'end': 3.96,\n",
       "    'text': '<|notimestamps|> The streets were narrow and unpaid but very fairly clean.',\n",
       "    'tokens': array([  440,  8481,   645,  9432,   293,   517, 35035,   457,   588,\n",
       "            6457,  2541,    13], dtype=int32),\n",
       "    'lang': 'en',\n",
       "    'time': 3.96}],\n",
       "  'lang': 'en',\n",
       "  'filename': 'audio_en.wav'}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from loggers import set_level\n",
    "from models.stt import transcribe\n",
    "from utils.audio import display_audio\n",
    "\n",
    "set_level('time')\n",
    "\n",
    "filename = 'audio_en.wav'\n",
    "pred = transcribe(filename, model = 'whisper-base', save = False, lang = 'en', display = False)\n",
    "\n",
    "#display_audio(filename)\n",
    "pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorRT-LLM inference\n",
    "\n",
    "The `transcribe` method is the same for all the supported models. Simply provide the `model`'s name, and it will be automatically loaded, no matter it is a `keras` or `TensorRT-LLM`-based model ;)\n",
    "\n",
    "**Note** : It is worth mentioning that, in the above example, the `whisper-base` model is used, while in the below cell, it is a fine-tuned version of the `large-v3` model. The inference time are therefore **not** comparable, as the last one is **much bigger**. \n",
    "\n",
    "**Known limitation** : currently, the `Whisper` engine predicts by default 1 token, no matter the parameters given to the build method. To overcome this, it is required to manually specify the `max_new_tokens` argument. In addition to that, the warnings displayed about the `CrossAttentionMask` were not displayed in the `v0.14`, but are actually not important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory limited to 512Mb\n",
      "[TensorRT-LLM][WARNING] Default padding attention mask will be used as not all requests have cross attention mask.\n",
      "[TensorRT-LLM][WARNING] CrossAttentionMask is not provided for the request. Default padding attention mask will be created.\n",
      "[TensorRT-LLM][WARNING] CrossAttentionMask is not provided for the generation request. Full valid attentionMask will be used by default.\n",
      "[TensorRT-LLM][WARNING] CrossAttentionMask is not provided for the generation request. Full valid attentionMask will be used by default.\n",
      "[TensorRT-LLM][WARNING] CrossAttentionMask is not provided for the generation request. Full valid attentionMask will be used by default.\n",
      "[TensorRT-LLM][WARNING] CrossAttentionMask is not provided for the generation request. Full valid attentionMask will be used by default.\n",
      "[TensorRT-LLM][WARNING] CrossAttentionMask is not provided for the generation request. Full valid attentionMask will be used by default.\n",
      "[TensorRT-LLM][WARNING] CrossAttentionMask is not provided for the generation request. Full valid attentionMask will be used by default.\n",
      "[TensorRT-LLM][WARNING] CrossAttentionMask is not provided for the generation request. Full valid attentionMask will be used by default.\n",
      "[TensorRT-LLM][WARNING] CrossAttentionMask is not provided for the generation request. Full valid attentionMask will be used by default.\n",
      "[TensorRT-LLM][WARNING] CrossAttentionMask is not provided for the generation request. Full valid attentionMask will be used by default.\n",
      "[TensorRT-LLM][WARNING] CrossAttentionMask is not provided for the generation request. Full valid attentionMask will be used by default.\n",
      "[TensorRT-LLM][WARNING] CrossAttentionMask is not provided for the generation request. Full valid attentionMask will be used by default.\n",
      "[TensorRT-LLM][WARNING] CrossAttentionMask is not provided for the generation request. Full valid attentionMask will be used by default.\n",
      "[TensorRT-LLM][WARNING] CrossAttentionMask is not provided for the generation request. Full valid attentionMask will be used by default.\n",
      "[TensorRT-LLM][WARNING] CrossAttentionMask is not provided for the generation request. Full valid attentionMask will be used by default.\n",
      "14 tokens generated in 95 ms (146.052 tokens/sec)\n",
      "Timers :\n",
      "- predict : 120 ms\n",
      "  - predict : 120 ms\n",
      "    - inference : 120 ms\n",
      "      - loading audio : 22 ms\n",
      "        - read_audio : 21 ms\n",
      "          - read file : 20 ms\n",
      "          - normalize_audio : 239 μs\n",
      "          - trim_silence : 236 μs\n",
      "        - mel spectrogram : 1.191 ms\n",
      "          - graph_mel_spectrogram : 603 μs\n",
      "      - segment processing : 465 μs\n",
      "      - TRT-LLM inference : 97 ms\n",
      "      - post_processing : 132 μs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'text': 'The streets were narrow and unpaved, but very fairly clean.',\n",
       "  'segments': [{'start': 0.0,\n",
       "    'end': 3.96,\n",
       "    'text': 'The streets were narrow and unpaved, but very fairly clean.',\n",
       "    'tokens': array([ 2278,  8481,   645,  9432,   293, 20994, 12865,    11,   457,\n",
       "             588,  6457,  2541,    13], dtype=int32),\n",
       "    'lang': 'en',\n",
       "    'time': 3.96}],\n",
       "  'lang': 'en',\n",
       "  'filename': 'audio_en.wav'}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from loggers import set_level\n",
    "from models.stt import transcribe\n",
    "from utils.audio import display_audio\n",
    "from utils import setup_environment\n",
    "\n",
    "setup_environment(gpu_memory = 512)\n",
    "\n",
    "set_level('time')\n",
    "\n",
    "filename = 'audio_en.wav'\n",
    "pred = transcribe(\n",
    "    filename, model = 'trt_llm_whisper_large_v3_french_int8', save = False, lang = 'en', display = False, max_new_tokens = 128\n",
    ")\n",
    "\n",
    "#display_audio(filename)\n",
    "pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search keyword in audio\n",
    "\n",
    "**This feature comes from an older version and is not working anymore. This will be updated for the next release**\n",
    "\n",
    "The `search` function allows to search a keyword in an audio / video, and get all timestamps where this keyword has been found (with a given probability threshold). Even though the model is quite accurate, it can make some spelling mistakes, like in the given audio (i.e., the *unpaid* should be *unpaved*). To mitigate this in the matching function, the `Edit` distance (aka `Levenshtein distance`) is used with a *partial* alignment to find all occurences with a given tolerance. \n",
    "\n",
    "Once the positions of the candidates have been found, its approximate timestamp is provided based on its relative position, and the time information of the segments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model name : whisper-base\n",
      "Result for searching keyword 'clean' :\n",
      "Number of files : 1 / 1\n",
      "Total number of occurences : 1\n",
      "Files : Annotation of file audio_en.wav :\n",
      "- Total annotation time : 30.000 sec\n",
      "- Number of alignments : 1 (1 sub-parts)\n",
      "- Speakers (n = 1) : [-1]\n",
      "\n",
      "Occurences of 'clean' (1, threshold = 80.00%) :\n",
      "- Timestamp 26.316 sec (p = 100.00 %) : [...]  clean. [...]\n",
      "\n",
      "pretrained_models/whisper-base/search/map.json\n",
      "Filename is in processed file : False\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from utils import load_json\n",
    "from models import get_model_dir\n",
    "from models.stt import search, get_model_name\n",
    "\n",
    "model_name = 'whisper-base'\n",
    "filename = 'audio_en.wav'\n",
    "\n",
    "print(\"Model name : {}\".format(model_name))\n",
    "r = search('clean', filename, model = model_name)\n",
    "print(r)\n",
    "print(get_model_dir(model_name, 'search', 'map.json'))\n",
    "print(\"Filename is in processed file : {}\".format(filename in load_json(get_model_dir(model_name, 'outputs', 'map.json'))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r.display(before = 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Edit distance demonstration\n",
    "\n",
    "This example illustrates with longer example the **edit distance** with partial alignment for searching keyword in bad-spelled text (as described in the README file). \n",
    "\n",
    "The objective is to find *cat* in the text *the ct is here*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Edit distance without partial alignment :\n",
      "          t    h    e         c    t         i    s           h     e     r     e\n",
      "   0.0  1.0  2.0  3.0  4.0  5.0  6.0  7.0  8.0  9.0  10.0  11.0  12.0  13.0  14.0\n",
      "c  1.0  1.0  2.0  3.0  4.0  4.0  5.0  6.0  7.0  8.0   9.0  10.0  11.0  12.0  13.0\n",
      "a  2.0  2.0  2.0  3.0  4.0  5.0  5.0  6.0  7.0  8.0   9.0  10.0  11.0  12.0  13.0\n",
      "t  3.0  2.0  3.0  3.0  4.0  5.0  5.0  6.0  7.0  8.0   9.0  10.0  11.0  12.0  13.0\n",
      "Edit distance with partial alignment :\n",
      "          t    h    e         c    t         i    s         h    e    r    e\n",
      "   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
      "c  1.0  1.0  1.0  1.0  1.0  0.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0\n",
      "a  2.0  2.0  2.0  2.0  2.0  1.0  1.0  2.0  2.0  2.0  2.0  2.0  2.0  2.0  2.0\n",
      "t  3.0  2.0  3.0  3.0  3.0  2.0  1.0  2.0  3.0  3.0  3.0  3.0  3.0  3.0  3.0\n",
      "Best alignment :  ct\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from loggers import set_level\n",
    "from utils import plot, set_display_options\n",
    "from utils.text import edit_distance\n",
    "\n",
    "set_level('info')\n",
    "set_display_options()\n",
    "\n",
    "truth = 'the ct is here'\n",
    "hypothesis = 'cat'\n",
    "\n",
    "print(\"Edit distance without partial alignment :\")\n",
    "dist, matrix = edit_distance(hypothesis, truth, partial = False, return_matrix = True, normalize = False, verbose = True)\n",
    "\n",
    "print(\"Edit distance with partial alignment :\")\n",
    "partial_dist, partial_matrix = edit_distance(hypothesis, truth, partial = True, return_matrix = True, normalize = False, verbose = True)\n",
    "\n",
    "start_idx = np.argmin(partial_matrix[-1, 1:]) + 1 - len(hypothesis)\n",
    "print(\"Best alignment : {}\".format(truth[start_idx : start_idx + len(hypothesis)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "probability = 1. - partial_matrix[-1, 1:] / len(hypothesis)\n",
    "plot(probability, ylim = (0, 1), xlabel = \"caracter index\", ylabel = \"probability (%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
